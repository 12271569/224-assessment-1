{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76fb37c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (7613, 5)\n",
      "id             0\n",
      "keyword       61\n",
      "location    2533\n",
      "text           0\n",
      "target         0\n",
      "dtype: int64\n",
      "Training data shape: (6090, 100)\n",
      "Testing data shape: (1523, 100)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Step1: Data Cleaning and Preprocessing\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "print(\"Original dataset shape:\", df.shape)\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Fill missing values in 'keyword' or 'location'\n",
    "df['text'] = df['text'].fillna(\"\")\n",
    "\n",
    "# Clean the tweet text\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\@w+|\\#','', text)   # Remove mentions and hashtags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['clean_text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['clean_text'])\n",
    "\n",
    "# Padding\n",
    "max_length = 100\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Labels\n",
    "labels = df['target'].values\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Testing data shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fa8485a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Study\\AUS\\AUSWORK 2025\\April 2025\\CQU\\Dhruvi\\COIT29224_Assignment1\\Code\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191/191 - 2s - 10ms/step - accuracy: 0.5672 - loss: 0.6828 - val_accuracy: 0.5739 - val_loss: 0.6822\n",
      "Epoch 2/10\n",
      "191/191 - 1s - 3ms/step - accuracy: 0.5706 - loss: 0.6795 - val_accuracy: 0.5739 - val_loss: 0.6743\n",
      "Epoch 3/10\n",
      "191/191 - 1s - 4ms/step - accuracy: 0.5798 - loss: 0.6687 - val_accuracy: 0.6369 - val_loss: 0.6580\n",
      "Epoch 4/10\n",
      "191/191 - 1s - 4ms/step - accuracy: 0.6695 - loss: 0.6346 - val_accuracy: 0.6428 - val_loss: 0.6122\n",
      "Epoch 5/10\n",
      "191/191 - 1s - 4ms/step - accuracy: 0.7447 - loss: 0.5634 - val_accuracy: 0.7498 - val_loss: 0.5463\n",
      "Epoch 6/10\n",
      "191/191 - 1s - 4ms/step - accuracy: 0.7898 - loss: 0.4917 - val_accuracy: 0.7827 - val_loss: 0.5040\n",
      "Epoch 7/10\n",
      "191/191 - 1s - 4ms/step - accuracy: 0.8217 - loss: 0.4329 - val_accuracy: 0.7991 - val_loss: 0.4706\n",
      "Epoch 8/10\n",
      "191/191 - 1s - 4ms/step - accuracy: 0.8394 - loss: 0.3969 - val_accuracy: 0.8024 - val_loss: 0.4566\n",
      "Epoch 9/10\n",
      "191/191 - 1s - 4ms/step - accuracy: 0.8501 - loss: 0.3697 - val_accuracy: 0.7853 - val_loss: 0.4887\n",
      "Epoch 10/10\n",
      "191/191 - 1s - 4ms/step - accuracy: 0.8680 - loss: 0.3386 - val_accuracy: 0.7997 - val_loss: 0.4584\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7737 - loss: 0.4837\n",
      "\n",
      "Test Accuracy: 0.7997\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Step 2: Baseline Neural Network with Manual Tuning\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define hyperparameters\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 16\n",
    "max_length = 100\n",
    "dropout_rate = 0.5\n",
    "dense_units = 24\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Build the baseline model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(dense_units, activation='relu'),\n",
    "    Dropout(dropout_rate),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "# Evaluate on test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e2f784c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1: Best Accuracy = 0.8021\n",
      "Generation 2: Best Accuracy = 0.8021\n",
      "Generation 3: Best Accuracy = 0.8071\n",
      "Generation 4: Best Accuracy = 0.8103\n",
      "Generation 5: Best Accuracy = 0.8103\n",
      "Generation 6: Best Accuracy = 0.8103\n",
      "Generation 7: Best Accuracy = 0.8103\n",
      "Generation 8: Best Accuracy = 0.8112\n",
      "Generation 9: Best Accuracy = 0.8112\n",
      "Generation 10: Best Accuracy = 0.8112\n",
      "\n",
      "Optimization Complete!\n",
      "Best Learning Rate: 0.01000\n",
      "Best Dense Units: 55\n",
      "Best Dropout Rate: 0.4466\n",
      "Validation Accuracy: 0.8112\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Step 3: PSO-Optimized\tNeural\tNetwork\tImplementation\"\"\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Dataset split\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# PSO CONFIGURATION\n",
    "SWARM_SIZE = 10\n",
    "DIMENSIONS = 3  # [learning_rate, dense_units, dropout_rate]\n",
    "INFORMANTS = 3\n",
    "NUM_GENERATIONS = 10\n",
    "W = 0.729\n",
    "C1 = 1.49\n",
    "C2 = 1.49\n",
    "desired_precision = 1e-5\n",
    "\n",
    "# BOUNDS for [learning_rate, dense_units, dropout_rate]\n",
    "MIN_BOUNDARY = [0.0005, 16, 0.2]\n",
    "MAX_BOUNDARY = [0.01, 128, 0.6]\n",
    "\n",
    "# Fitness function (returns 1 - accuracy, as we want to minimize)\n",
    "def fitness_function(position):\n",
    "    lr = position[0]\n",
    "    dense_units = int(position[1])\n",
    "    dropout = float(position[2])\n",
    "    \n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=48, input_length=100),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dropout(dropout),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train_split, y_train_split, epochs=5, batch_size=32,\n",
    "                        validation_data=(X_val, y_val), verbose=0)\n",
    "    acc = history.history['val_accuracy'][-1]\n",
    "    return 1 - acc  # minimize error\n",
    "\n",
    "# Particle class\n",
    "class Particle:\n",
    "    def __init__(self):\n",
    "        self.position = [\n",
    "            random.uniform(MIN_BOUNDARY[0], MAX_BOUNDARY[0]),\n",
    "            random.uniform(MIN_BOUNDARY[1], MAX_BOUNDARY[1]),\n",
    "            random.uniform(MIN_BOUNDARY[2], MAX_BOUNDARY[2])\n",
    "        ]\n",
    "        self.velocity = [random.uniform(-1, 1) for _ in range(DIMENSIONS)]\n",
    "        self.fitness = fitness_function(self.position)\n",
    "        self.best_position = list(self.position)\n",
    "        self.best_fitness = self.fitness\n",
    "        self.informants = random.sample(range(SWARM_SIZE), INFORMANTS)\n",
    "        self.group_best_position = list(self.position)\n",
    "        self.group_best_fitness = self.fitness\n",
    "\n",
    "    def update_velocity(self):\n",
    "        for d in range(DIMENSIONS):\n",
    "            r1, r2 = random.random(), random.random()\n",
    "            cognitive = C1 * r1 * (self.best_position[d] - self.position[d])\n",
    "            social = C2 * r2 * (self.group_best_position[d] - self.position[d])\n",
    "            self.velocity[d] = W * self.velocity[d] + cognitive + social\n",
    "\n",
    "    def update_position(self):\n",
    "        for d in range(DIMENSIONS):\n",
    "            self.position[d] += self.velocity[d]\n",
    "            self.position[d] = max(MIN_BOUNDARY[d], min(MAX_BOUNDARY[d], self.position[d]))\n",
    "        self.fitness = fitness_function(self.position)\n",
    "\n",
    "    def update_group_best(self, swarm):\n",
    "        best_informant = min(self.informants, key=lambda i: swarm[i].best_fitness)\n",
    "        if swarm[best_informant].best_fitness < self.group_best_fitness:\n",
    "            self.group_best_fitness = swarm[best_informant].best_fitness\n",
    "            self.group_best_position = list(swarm[best_informant].best_position)\n",
    "\n",
    "# Initialize swarm\n",
    "swarm = [Particle() for _ in range(SWARM_SIZE)]\n",
    "global_best = min(swarm, key=lambda p: p.best_fitness)\n",
    "global_best_position = list(global_best.best_position)\n",
    "global_best_fitness = global_best.best_fitness\n",
    "\n",
    "# PSO loop\n",
    "for gen in range(NUM_GENERATIONS):\n",
    "    for particle in swarm:\n",
    "        particle.update_group_best(swarm)\n",
    "        particle.update_velocity()\n",
    "        particle.update_position()\n",
    "        if particle.fitness < particle.best_fitness:\n",
    "            particle.best_fitness = particle.fitness\n",
    "            particle.best_position = list(particle.position)\n",
    "    best_particle = min(swarm, key=lambda p: p.best_fitness)\n",
    "    if best_particle.best_fitness < global_best_fitness:\n",
    "        global_best_fitness = best_particle.best_fitness\n",
    "        global_best_position = list(best_particle.best_position)\n",
    "\n",
    "    print(f\"Generation {gen+1}: Best Accuracy = {1 - global_best_fitness:.4f}\")\n",
    "\n",
    "    if global_best_fitness < desired_precision:\n",
    "        print(\"Desired precision reached.\")\n",
    "        break\n",
    "\n",
    "print(\"\\nOptimization Complete!\")\n",
    "print(f\"Best Learning Rate: {global_best_position[0]:.5f}\")\n",
    "print(f\"Best Dense Units: {int(global_best_position[1])}\")\n",
    "print(f\"Best Dropout Rate: {global_best_position[2]:.4f}\")\n",
    "print(f\"Validation Accuracy: {1 - global_best_fitness:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f349bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Study\\AUS\\AUSWORK 2025\\April 2025\\CQU\\Dhruvi\\COIT29224_Assignment1\\Code\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      " Model Accuracy Comparison:\n",
      "Baseline Model Accuracy     : 0.7997\n",
      "PSO-Optimized Model Accuracy: 0.8089\n",
      "\n",
      " Paired t-test Results:\n",
      "t-statistic: 9.8123\n",
      "p-value    : 0.00000\n",
      " Difference is statistically significant — PSO model performs significantly better!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Step 4: Performance\tComparison\tand\tAnalysis\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Reassign the baseline model\n",
    "baseline_model = model  # this was already trained earlier\n",
    "\n",
    "# Rebuild the best PSO-optimized model from global_best_position\n",
    "best_embedding_dim = 48  # fixed in PSO\n",
    "best_dense_units = int(global_best_position[1])\n",
    "best_dropout = float(global_best_position[2])\n",
    "best_lr = float(global_best_position[0])\n",
    "\n",
    "best_model = Sequential([\n",
    "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=best_embedding_dim, input_length=100),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(best_dense_units, activation='relu'),\n",
    "    Dropout(best_dropout),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "best_model.compile(optimizer=Adam(learning_rate=best_lr),\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train the PSO model on full training data\n",
    "best_model.fit(X_train, y_train, epochs=10, batch_size=32,\n",
    "               validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "# Predictions\n",
    "baseline_probs = baseline_model.predict(X_test)\n",
    "pso_probs = best_model.predict(X_test)\n",
    "\n",
    "y_pred_baseline = (baseline_probs > 0.5).astype(int).flatten()\n",
    "y_pred_pso = (pso_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# Accuracy comparison\n",
    "acc_baseline = accuracy_score(y_test, y_pred_baseline)\n",
    "acc_pso = accuracy_score(y_test, y_pred_pso)\n",
    "\n",
    "print(\" Model Accuracy Comparison:\")\n",
    "print(f\"Baseline Model Accuracy     : {acc_baseline:.4f}\")\n",
    "print(f\"PSO-Optimized Model Accuracy: {acc_pso:.4f}\")\n",
    "\n",
    "# Paired t-test\n",
    "t_stat, p_val = ttest_rel(y_pred_pso, y_pred_baseline)\n",
    "print(\"\\n Paired t-test Results:\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value    : {p_val:.5f}\")\n",
    "\n",
    "if p_val < 0.05:\n",
    "    print(\" Difference is statistically significant — PSO model performs significantly better!\")\n",
    "else:\n",
    "    print(\" No statistically significant difference — results are close.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
